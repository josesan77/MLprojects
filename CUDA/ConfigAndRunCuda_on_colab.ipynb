{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josesan77/MLprojects/blob/master/CUDA/ConfigAndRunCuda_on_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Prerequisites"
      ],
      "metadata": {
        "id": "tT4786xJBWKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all check/define GPU environment for the Notebook.\n",
        "1. In this Notebook's *Runtime* menu / *Change runtime type* submenu check GPU availability. For free accounts it should be T4 GPU (in Jan 2025), and be aware that GPU usage is limited, but enough for this calculation. Paying subscription can reach stronger hardware without time limitation, but the T4 GPU is enough now.\n",
        "2. Select T4 GPU\n",
        "3. Connect GPU at the top-right corner, below Settings icon) before starting session (processing codes below).\n",
        "\n",
        "[More on GPUs](https://cloud.google.com/compute/docs/gpus):\n",
        "\n",
        "V2-8 TPU is not needed!\n",
        "[V2 Pod slice with 8 TensorCores](https://cloud.google.com/tpu/docs/v2)"
      ],
      "metadata": {
        "id": "uQm1TyE3b7-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove - if required - existing CUDA installation and NVIDIA drivers"
      ],
      "metadata": {
        "id": "phO2W5bZqdHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, verify what NVCC version is installed on your Colab(oratory) environment."
      ],
      "metadata": {
        "id": "3V7ajLpUqdkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "gCk-tnDUEwKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is what you should get if `nvcc` is installed:\n",
        "\n",
        "```\n",
        "nvcc: NVIDIA (R) Cuda compiler driver\n",
        "Copyright (c) 2005-2018 NVIDIA Corporation\n",
        "Built on Wed_Apr_11_23:16:29_CDT_2018\n",
        "Cuda compilation tools, release 9.2, V9.2.88\n",
        "```\n",
        "\n",
        "Check release and version at the bottom! If it indicates release 9.2, V9.2.88 or higher (more recent) version then skipp section Remove/reinstall!\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uuRlQgy3E0aA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Remove/reinstall\n",
        "Perform the next steps only in case running the first cell code failed!\n",
        "### 1. Remove CUDA and NVIDIA modules"
      ],
      "metadata": {
        "id": "rf_BKy-HFjEs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQtstBaofzYT"
      },
      "outputs": [],
      "source": [
        "!apt-get --purge remove cuda nvidia* libnvidia-*\n",
        "!dpkg -l | grep cuda- | awk '{print $2}' | xargs -n1 dpkg --purge\n",
        "!apt-get remove cuda-*\n",
        "!apt autoremove\n",
        "!apt-get update"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Install specific CUDA"
      ],
      "metadata": {
        "id": "6t2vFrA3qlE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64 -O cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda-9.2"
      ],
      "metadata": {
        "id": "Othydrgyf-qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Installation check"
      ],
      "metadata": {
        "id": "8SnWeOi9qrNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA1C6v_JhI0E",
        "outputId": "2f860556-f3bf-4b23-9a7d-0325d597a14c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install a jupyter extension\n",
        "Skipping or performing the previous section, first run a test whether module is already installed. In general it is not, in that case install plugin using the code in the next cell.\n",
        "When running\n",
        "```\n",
        "%load_ext nvcc_plugin\n",
        "```\n",
        "the code should return:\n",
        "```\n",
        "created output directory at /content/src\n",
        "Out bin /content/result.out\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "SzaCN9poqvf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#instead of %load_ext nvcc_plugin #an earlier version\n",
        "%load_ext nvcc4jupyter # worked in Jan 2025, see later successful load"
      ],
      "metadata": {
        "id": "ZhVxPtS-F8b2",
        "outputId": "09847c26-6e92-4716-fe19-4d80dca4ab6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'nvcc_plugin'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5df49c27db81>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nvcc_plugin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-57>\u001b[0m in \u001b[0;36mload_ext\u001b[0;34m(self, module_str)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/extension.py\u001b[0m in \u001b[0;36mload_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Missing module name.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'already loaded'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/extensions.py\u001b[0m in \u001b[0;36mload_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                         print((\"Loading extensions from {dir} is deprecated. \"\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nvcc_plugin'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If above code fails, let's instal the required module. In general Colaboratory can reach appropriate pip modules to instal, so try this first:"
      ],
      "metadata": {
        "id": "IRqySqYkdfWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvcc4jupyter"
      ],
      "metadata": {
        "id": "fVQ7gbAfKPde",
        "outputId": "121c92d8-13ab-45ac-e9b1-69a6ba8ddd5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Successful installation returns this:\n",
        "```\n",
        "Collecting nvcc4jupyter\n",
        "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
        "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
        "Installing collected packages: nvcc4jupyter\n",
        "Successfully installed nvcc4jupyter-1.2.1\n",
        "```\n",
        "If he above defined installation method fails, try this:"
      ],
      "metadata": {
        "id": "PEYU-PM5dpKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "2LjqG42cgdQQ",
        "outputId": "a297d33f-6cb9-4634-85e7-c4e0fc3274d5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "A UTF-8 locale is required. Got ANSI_X3.4-1968",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-8ef7b33a8aea>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install git+git://github.com/andreinechaev/nvcc4jupyter.git'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case both above pip instals fail, then clone the appropriate git library, enter the folder and install from there"
      ],
      "metadata": {
        "id": "Pt12V9F7duI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "id": "co8Vlg0RJYf-",
        "outputId": "44550bf4-b49e-4d5e-c6fa-fc62cff821f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nvcc4jupyter'...\n",
            "remote: Enumerating objects: 529, done.\u001b[K\n",
            "remote: Counting objects: 100% (408/408), done.\u001b[K\n",
            "remote: Compressing objects: 100% (207/207), done.\u001b[K\n",
            "remote: Total 529 (delta 209), reused 315 (delta 179), pack-reused 121 (from 1)\u001b[K\n",
            "Receiving objects: 100% (529/529), 120.01 KiB | 6.32 MiB/s, done.\n",
            "Resolving deltas: 100% (247/247), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd nvcc4jupyter/\n",
        "#ls #list files in folder, if needed"
      ],
      "metadata": {
        "id": "muKn73qVJpCX",
        "outputId": "500472e5-1754-4675-cc7d-00f76639818e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nvcc4jupyter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the module. Find the solution!"
      ],
      "metadata": {
        "id": "ouUrHvPtsPxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# verify state of the cloned git, it is fine if \"nothing to commit, working tree clean\"\n",
        "!git status"
      ],
      "metadata": {
        "id": "9xisWpL3KLqE",
        "outputId": "f41c98c8-a83b-43c4-e69d-8dc6c1e57dba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch master\n",
            "Your branch is up to date with 'origin/master'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the plugin"
      ],
      "metadata": {
        "id": "tabn9_zlqyx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#instead of %load_ext nvcc_plugin\n",
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uVGJjHrhdOu",
        "outputId": "7806feaf-4e0c-42f6-c865-4bf2243db98a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpkvda_pct\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Successful load results in (something similar):\n",
        "```\n",
        "Detected platform \"Colab\". Running its setup...\n",
        "Source files will be saved in \"/tmp/tmp__erspob\".\n",
        "```"
      ],
      "metadata": {
        "id": "WsYXkIbneaSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run CUDA"
      ],
      "metadata": {
        "id": "lgumuFZuBsw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run a print example\n",
        "Print the text \"This is from CUDA\" with a new line at the end."
      ],
      "metadata": {
        "id": "v0COdZH9q0lV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nvcc_plugin worked with magic(lin): %%cu  use\n",
        "%%cuda\n",
        "#include <iostream>\n",
        "\n",
        "int main() {\n",
        "    std::cout << \"This is from CUDA\\n\";\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfhvU2Ghhh_U",
        "outputId": "5d3bd2b6-93b8-4b84-fced-62fc7b540964"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is from CUDA\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UsageError: Cell magic `%%cu` not found."
      ],
      "metadata": {
        "id": "tfklraqkNdSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Involved example\n",
        "C++ code is designed to find the maximum value in an array using CUDA for parallel computation."
      ],
      "metadata": {
        "id": "rkbs-IDaB14q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <cstdio>\n",
        "#include <iostream>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "__global__ void maxi(int* a, int* b, int n)\n",
        "{\n",
        "\tint block = 256 * blockIdx.x;\n",
        "\tint max = 0;\n",
        "\n",
        "\tfor (int i = block; i < min(256 + block, n); i++) {\n",
        "\n",
        "\t\tif (max < a[i]) {\n",
        "\t\t\tmax = a[i];\n",
        "\t\t}\n",
        "\t}\n",
        "\tb[blockIdx.x] = max;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "\n",
        "\tint n;\n",
        "\tn = 3 >> 2;\n",
        "\tint a[n];\n",
        "\n",
        "\tfor (int i = 0; i < n; i++) {\n",
        "\t\ta[i] = rand() % n;\n",
        "\t\tcout << a[i] << \"\\t\";\n",
        "\t}\n",
        "\n",
        "\tcudaEvent_t start, end;\n",
        "\tint *ad, *bd;\n",
        "\tint size = n * sizeof(int);\n",
        "\tcudaMalloc(&ad, size);\n",
        "\tcudaMemcpy(ad, a, size, cudaMemcpyHostToDevice);\n",
        "\tint grids = ceil(n * 1.0f / 256.0f);\n",
        "\tcudaMalloc(&bd, grids * sizeof(int));\n",
        "\n",
        "\tdim3 grid(grids, 1);\n",
        "\tdim3 block(1, 1);\n",
        "\n",
        "\tcudaEventCreate(&start);\n",
        "\tcudaEventCreate(&end);\n",
        "\tcudaEventRecord(start);\n",
        "\n",
        "\twhile (n > 1) {\n",
        "\t\tmaxi<<<grids, block>>>(ad, bd, n);\n",
        "\t\tn = ceil(n * 1.0f / 256.0f);\n",
        "\t\tcudaMemcpy(ad, bd, n * sizeof(int), cudaMemcpyDeviceToDevice);\n",
        "\t}\n",
        "\n",
        "\tcudaEventRecord(end);\n",
        "\tcudaEventSynchronize(end);\n",
        "\n",
        "\tfloat time = 0;\n",
        "\tcudaEventElapsedTime(&time, start, end);\n",
        "\n",
        "\tint ans[2];\n",
        "\tcudaMemcpy(ans, ad, 4, cudaMemcpyDeviceToHost);\n",
        "\n",
        "\tcout << \"The maximum element is : \" << ans[0] << endl;\n",
        "\n",
        "\tcout << \"The time required : \";\n",
        "\tcout << time << endl;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTOfhmfwh3s4",
        "outputId": "140aaa37-a38d-4007-a523-ec5c9963d166"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The maximum element is : 0\n",
            "The time required : 0.002976\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try with Python\n",
        "Since Python doesn't have built-in support for CUDA like C++, the Python equivalent will use libraries like NumPy for array operations and CuPy for CUDA-related tasks."
      ],
      "metadata": {
        "id": "wYzJXAj1PB-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time # checking runtime"
      ],
      "metadata": {
        "id": "4uZDl1abPA4f"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "try simple python without CUDA"
      ],
      "metadata": {
        "id": "vjPB-vaTPHPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maxi_kernel_python(a, b, n):\n",
        "    \"\"\"\n",
        "    Python implementation of the CUDA kernel for finding the maximum in each block.\n",
        "    \"\"\"\n",
        "    threads_per_block = 256\n",
        "    grid_size = len(b)\n",
        "    for block_idx in range(grid_size):\n",
        "        block_start = threads_per_block * block_idx\n",
        "        block_end = min(block_start + threads_per_block, n)\n",
        "        max_val = 0\n",
        "        for i in range(block_start, block_end):\n",
        "            max_val = max(max_val, a[i])\n",
        "        b[block_idx] = max_val\n",
        "\n",
        "def main_python():\n",
        "    # Initialize data\n",
        "    start = time.time()\n",
        "    n = 3 >> 2\n",
        "    a = np.random.randint(0, n, n, dtype=np.int32)\n",
        "    print(\"Array:\", a)\n",
        "\n",
        "    # Allocate device memory\n",
        "    ad = cp.array(a)\n",
        "    grids = int(cp.ceil(n / 256))\n",
        "    bd = cp.zeros(grids, dtype=cp.int32)\n",
        "\n",
        "    while n > 1:\n",
        "        # Create host copies for kernel emulation\n",
        "        a_host = ad.get()\n",
        "        b_host = np.zeros_like(bd.get())\n",
        "\n",
        "        # Call the kernel function (Python version)\n",
        "        maxi_kernel_python(a_host, b_host, n)\n",
        "\n",
        "        # Transfer results back to GPU\n",
        "        bd = cp.array(b_host)\n",
        "        ad = bd\n",
        "        n = len(bd)\n",
        "\n",
        "    # Get results\n",
        "    ans = ad[0].item() # ad.get()[0]\n",
        "    end = time.time()\n",
        "    elapsed_time = (end- start)\n",
        "\n",
        "    print(\"The maximum element is:\", ans)\n",
        "    print(\"The time required:\", elapsed_time, \"ms\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "vBkJeSZfPKQ0",
        "outputId": "a1bafcbc-911d-4cac-81f3-69c5a3303d6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array: []\n",
            "The maximum element is: []\n",
            "The time required: 0.0025017261505126953 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "# CUDA kernel to find the maximum in each block\n",
        "maxi_kernel = cp.RawKernel(r'''\n",
        "extern \"C\" __global__\n",
        "void maxi(const int* a, int* b, int n) {\n",
        "    int block = 256 * blockIdx.x;\n",
        "    int max = 0;\n",
        "\n",
        "    for (int i = block; i < min(256 + block, n); i++) {\n",
        "        if (max < a[i]) {\n",
        "            max = a[i];\n",
        "        }\n",
        "    }\n",
        "    b[blockIdx.x] = max;\n",
        "}\n",
        "''', 'maxi')\n",
        "\n",
        "def main():\n",
        "    # Initialize data\n",
        "    n = 3 >> 2\n",
        "    a = np.random.randint(0, n, n, dtype=np.int32)\n",
        "    print(\"Array:\", a)\n",
        "\n",
        "    # Allocate device memory\n",
        "    ad = cp.array(a)\n",
        "    grids = int(cp.ceil(n / 256))\n",
        "    bd = cp.zeros(grids, dtype=cp.int32)\n",
        "\n",
        "    # Measure execution time\n",
        "    start = cp.cuda.Event()\n",
        "    end = cp.cuda.Event()\n",
        "    start.record()\n",
        "\n",
        "    while n > 1:\n",
        "        maxi_kernel((grids,), (1,), (ad, bd, n))\n",
        "        n = int(cp.ceil(n / 256))\n",
        "        ad = cp.array(bd[:n])  # Copy results back to ad for the next iteration\n",
        "\n",
        "    end.record()\n",
        "    end.synchronize()\n",
        "\n",
        "    # Get results\n",
        "    ans = ad\n",
        "    time = cp.cuda.get_elapsed_time(start, end)\n",
        "\n",
        "    print(\"The maximum element is:\", ans)\n",
        "    print(\"The time required:\", time, \"ms\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bllfmprOOew2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db33c849-7d32-48f4-dedf-8558580418f3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array: []\n",
            "The maximum element is: []\n",
            "The time required: 0.003776000114157796 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1"
      ],
      "metadata": {
        "id": "7NKE8Zd8kH3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "// This is a special function that runs on the GPU (device) instead of the CPU (host)\n",
        "__global__ void kernel() {\n",
        "  printf(\"Hello world!\\n\");\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // Invoke the kernel function on the GPU with one block of one thread\n",
        "  kernel<<<1,1>>>();\n",
        "\n",
        "  // Check for error codes (remember to do this for _every_ CUDA function)\n",
        "  if(cudaDeviceSynchronize() != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Error: %s\\n\", cudaGetErrorString(cudaPeekAtLastError()));\n",
        "  }\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoRsWBb-kHWn",
        "outputId": "d2a9ceab-d030-4e25-9ec9-c97b9fb23635"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Hello world!\" text returned."
      ],
      "metadata": {
        "id": "hHaHKSdEjWdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2"
      ],
      "metadata": {
        "id": "dZHP7I8yB_Xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "// This kernel runs on the GPU and prints the thread's identifiers\n",
        "__global__ void kernel() {\n",
        "  printf(\"Hello from block %d thread %d\\n\", blockIdx.x, threadIdx.x);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // Launch the kernel on the GPU with four blocks of six threads each\n",
        "  kernel<<<4,6>>>();\n",
        "\n",
        "  // Check for CUDA errors\n",
        "  if(cudaDeviceSynchronize() != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Error: %s\\n\", cudaGetErrorString(cudaPeekAtLastError()));\n",
        "  }\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKhIVZ2Ckdyb",
        "outputId": "ac1527a6-d0ca-475f-a302-b0e78280d383"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from block 1 thread 0\n",
            "Hello from block 1 thread 1\n",
            "Hello from block 1 thread 2\n",
            "Hello from block 1 thread 3\n",
            "Hello from block 1 thread 4\n",
            "Hello from block 1 thread 5\n",
            "Hello from block 0 thread 0\n",
            "Hello from block 0 thread 1\n",
            "Hello from block 0 thread 2\n",
            "Hello from block 0 thread 3\n",
            "Hello from block 0 thread 4\n",
            "Hello from block 0 thread 5\n",
            "Hello from block 2 thread 0\n",
            "Hello from block 2 thread 1\n",
            "Hello from block 2 thread 2\n",
            "Hello from block 2 thread 3\n",
            "Hello from block 2 thread 4\n",
            "Hello from block 2 thread 5\n",
            "Hello from block 3 thread 0\n",
            "Hello from block 3 thread 1\n",
            "Hello from block 3 thread 2\n",
            "Hello from block 3 thread 3\n",
            "Hello from block 3 thread 4\n",
            "Hello from block 3 thread 5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A block of\n",
        "```\n",
        "Hello from block 1 thread 0\n",
        "Hello from block 1 thread 1\n",
        "Hello from block 1 thread 2\n",
        "Hello from block 1 thread 3\n",
        "Hello from block 1 thread 4\n",
        "Hello from block 1 thread 5\n",
        "Hello from block 0 thread 0\n",
        "Hello from block 0 thread 1\n",
        "Hello from block 0 thread 2\n",
        "...\n",
        "```\n",
        "is returned."
      ],
      "metadata": {
        "id": "Eoszz7bnjeb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3\n",
        "The following (CUDA) code implements a simplified version of the SAXPY operation (y = a * x + y) on a GPU. Here is a detailed breakdown:"
      ],
      "metadata": {
        "id": "gDACDMfTCDs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <stdint.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "#define N 32\n",
        "#define THREADS_PER_BLOCK 32\n",
        "\n",
        "// This kernel runs on the GPU and performs the SAXPY operation (kernel)\n",
        "__global__ void saxpy(float a, float* x, float* y) {\n",
        "  // Which index of the array should this thread use?\n",
        "  size_t index = 20; //hardcoded thread index for simplicity\n",
        "\n",
        "  // Compute a times x plus y for a specific index\n",
        "  y[index] = a * x[index] + y[index];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // Allocate arrays for X and Y on the CPU. This memory is only usable on the CPU\n",
        "  float* cpu_x = (float*)malloc(sizeof(float) * N);\n",
        "  float* cpu_y = (float*)malloc(sizeof(float) * N);\n",
        "\n",
        "  // Initialize X and Y\n",
        "  int i;\n",
        "  for(i=0; i<N; i++) {\n",
        "    cpu_x[i] = (float)i;\n",
        "    cpu_y[i] = 0.0;\n",
        "  }\n",
        "\n",
        "  // The gpu_x and gpu_y pointers will only be usable on the GPU (which uses separate memory)\n",
        "  float* gpu_x;\n",
        "  float* gpu_y;\n",
        "\n",
        "  // Allocate space for the x array on the GPU\n",
        "  if(cudaMalloc(&gpu_x, sizeof(float) * N) != cudaSuccess) {\n",
        "    fprintf(stderr, \"Failed to allocate X array on GPU\\n\");\n",
        "    exit(2);\n",
        "  }\n",
        "\n",
        "  // Allocate space for the y array on the GPU\n",
        "  if(cudaMalloc(&gpu_y, sizeof(float) * N) != cudaSuccess) {\n",
        "    fprintf(stderr, \"Failed to allocate Y array on GPU\\n\");\n",
        "    exit(2);\n",
        "  }\n",
        "\n",
        "  // Copy the cpu's x array to the gpu with cudaMemcpy, Data transfer from CPU to GPU\n",
        "  if(cudaMemcpy(gpu_x, cpu_x, sizeof(float) * N, cudaMemcpyHostToDevice) != cudaSuccess) {\n",
        "    fprintf(stderr, \"Failed to copy X to the GPU\\n\");\n",
        "  }\n",
        "\n",
        "  // Copy the cpu's y array to the gpu with cudaMemcpy, Data transfer from CPU to GPU\n",
        "  if(cudaMemcpy(gpu_y, cpu_y, sizeof(float) * N, cudaMemcpyHostToDevice) != cudaSuccess) {\n",
        "    fprintf(stderr, \"Failed to copy Y to the GPU\\n\");\n",
        "  }\n",
        "\n",
        "  // Calculate the number of blocks to run, rounding up to include all threads\n",
        "  size_t blocks = (N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n",
        "\n",
        "  // Run the saxpy kernel\n",
        "  saxpy<<<blocks, THREADS_PER_BLOCK>>>(0.5, gpu_x, gpu_y);\n",
        "\n",
        "  // Wait for the kernel to finish\n",
        "  if(cudaDeviceSynchronize() != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Error: %s\\n\", cudaGetErrorString(cudaPeekAtLastError()));\n",
        "  }\n",
        "\n",
        "  // Copy the y array back from the gpu to the cpu\n",
        "  if(cudaMemcpy(cpu_y, gpu_y, sizeof(float) * N, cudaMemcpyDeviceToHost) != cudaSuccess) {\n",
        "    fprintf(stderr, \"Failed to copy Y from the GPU\\n\");\n",
        "  }\n",
        "\n",
        "  // Print the updated y array (output)\n",
        "  for(i=0; i<N; i++) {\n",
        "    printf(\"%d: %f\\n\", i, cpu_y[i]);\n",
        "  }\n",
        "\n",
        "  //Cleanup\n",
        "  cudaFree(gpu_x);\n",
        "  cudaFree(gpu_y);\n",
        "  free(cpu_x);\n",
        "  free(cpu_y);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKAZmeMekgWp",
        "outputId": "145f4af7-ed34-449f-8703-1fbaea585140"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: 0.000000\n",
            "1: 0.000000\n",
            "2: 0.000000\n",
            "3: 0.000000\n",
            "4: 0.000000\n",
            "5: 0.000000\n",
            "6: 0.000000\n",
            "7: 0.000000\n",
            "8: 0.000000\n",
            "9: 0.000000\n",
            "10: 0.000000\n",
            "11: 0.000000\n",
            "12: 0.000000\n",
            "13: 0.000000\n",
            "14: 0.000000\n",
            "15: 0.000000\n",
            "16: 0.000000\n",
            "17: 0.000000\n",
            "18: 0.000000\n",
            "19: 0.000000\n",
            "20: 10.000000\n",
            "21: 0.000000\n",
            "22: 0.000000\n",
            "23: 0.000000\n",
            "24: 0.000000\n",
            "25: 0.000000\n",
            "26: 0.000000\n",
            "27: 0.000000\n",
            "28: 0.000000\n",
            "29: 0.000000\n",
            "30: 0.000000\n",
            "31: 0.000000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Key Observations (Example 3)\n",
        "1. Index Calculation Issue:\n",
        "\n",
        "The kernel only updates y[20] because the index is hardcoded, ignoring the thread ID (threadIdx.x) and block ID (blockIdx.x).\n",
        "This defeats the purpose of parallel computation.\n",
        "\n",
        "Reach parallel computation: Replace the hardcoded ```size_t index = 20``` with a calculation using thread and block indices:\n",
        "\n",
        "```\n",
        "size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "if (index < N) {\n",
        "    y[index] = a * x[index] + y[index];\n",
        "}\n",
        "```\n",
        "\n",
        "2. SAXPY Limitation:\n",
        "\n",
        "SAXPY is typically applied to all elements of the arrays in parallel. Here, the computation is limited to a single index.\n",
        "3. CUDA Memory Management:\n",
        "\n",
        "Proper allocation and transfer of memory between CPU and GPU are demonstrated."
      ],
      "metadata": {
        "id": "IQC7seAalDiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare Python and CUDA\n",
        "Let's look the runtime difference of a Python and a C++ code optimized for CUDA (GPU run)."
      ],
      "metadata": {
        "id": "5Afss_KlofJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 4: n-th number in the Fibonacci series\n",
        "Properly declared the below methodss would return only the n-th element of the Fibonacci series, just added print() for each step to demonstrate the functionality. (Yes, who knows, ... knows that printing takes \"lot of milliseconds\" :) )\n",
        "Let's start with CUDA / GPU."
      ],
      "metadata": {
        "id": "nXobjY8fudCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void fibonacci_kernel( int* fib, int n) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    if (idx == 0) {\n",
        "        fib[0] = 0;  // F(0)\n",
        "        fib[1] = 1;  // F(1)\n",
        "    }\n",
        "\n",
        "    __syncthreads();  // Ensure all threads initialize the first few Fibonacci values\n",
        "\n",
        "    if (idx == 0 && n > 2) {\n",
        "        for (int i = 2; i < n+1; i++) {\n",
        "            // Shift the registers and calculate the new Fibonacci value\n",
        "            int temp = fib[0] + fib[1];  // Calculate new Fibonacci number\n",
        "            fib[0] = fib[1];  // Shift fib[0]\n",
        "            fib[1] = temp;\n",
        "            printf(\"The %dth Fibonacci number is: %d\\n\", i, fib[1]);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    __syncthreads();  // Synchronize threads before final output\n",
        "\n",
        "    printf(\"The %dth Fibonacci number is: %d\\n\", n, fib[1]);\n",
        "\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int n = 50;  // Fibonacci sequence position (adjustable up to 50!)\n",
        "\n",
        "    int* host_fib = new int[3];  // We only need to store 3 values at any time\n",
        "    int* device_fib;\n",
        "\n",
        "    cudaMalloc((void**)&device_fib, 3 * sizeof(int));\n",
        "\n",
        "    // Initialize the first three Fibonacci numbers\n",
        "    cudaMemcpy(device_fib, host_fib, 3 * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threads_per_block = 1;\n",
        "    int blocks = 1;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    fibonacci_kernel<<<blocks, threads_per_block>>>(device_fib, n);\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaDeviceSynchronize();  // Ensure all threads are finished before continuing\n",
        "\n",
        "    float elapsed_time = 0.0;\n",
        "    cudaEventElapsedTime(&elapsed_time, start, stop);\n",
        "\n",
        "    // Print the elapsed time\n",
        "    printf(\"\\n Elapsed time: %.4f ms\\n\", elapsed_time);\n",
        "\n",
        "    cudaFree(device_fib);\n",
        "    delete[] host_fib;\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR-2YspcuiyN",
        "outputId": "ed68a492-c9a7-460b-e424-476e02f0dec7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Cell magic `%%cuda` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note: Writing to file & compiling\n",
        "If you want to write the code to a file for compiling, then use `%%writefile [filename]` and the C++ code"
      ],
      "metadata": {
        "id": "JfMZbZUQ2ThQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile fibonacci.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void fibonacci_kernel(int* fib, int n) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    if (idx == 0) fib[idx] = 0;  // First Fibonacci number\n",
        "    if (idx == 1) fib[idx] = 1;  // Second Fibonacci number\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (idx >= 2 && idx < n) {\n",
        "        fib[idx] = fib[idx - 1] + fib[idx - 2];\n",
        "        printf(\"Thread %d: %d\\n\", idx, fib[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int n = 10;\n",
        "    int* host_fib = new int[n];\n",
        "    int* device_fib;\n",
        "\n",
        "    host_fib[0] = 0;\n",
        "    host_fib[1] = 1;\n",
        "\n",
        "    cudaMalloc((void**)&device_fib, n * sizeof(int));\n",
        "    cudaMemcpy(device_fib, host_fib, n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threads_per_block = 32;\n",
        "    int blocks = (n + threads_per_block - 1) / threads_per_block;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    fibonacci_kernel<<<blocks, threads_per_block>>>(device_fib, n);\n",
        "    cudaEventRecord(stop);\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    float elapsed_time = 0.0;\n",
        "    cudaEventElapsedTime(&elapsed_time, start, stop);\n",
        "\n",
        "    cudaMemcpy(host_fib, device_fib, 2 * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"The 100th Fibonacci number is: %d\\n\", host_fib[n - 1]);\n",
        "    printf(\"Elapsed time: %.4f ms\\n\", elapsed_time);\n",
        "\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "    cudaFree(device_fib);\n",
        "    delete[] host_fib;\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ttz3PKJ2ObY",
        "outputId": "af9319c4-3530-4450-b255-8b01616078c6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting fibonacci.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check if the code is written to file fibonacci.cu\n",
        "%%sh\n",
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWSHwAwXyLPd",
        "outputId": "63159fe6-1956-48c2-c21d-d0557e581b1e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fibonacci.cu\n",
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check [this](https://colab.research.google.com/drive/1GJOfTp56OeQRdE4u2_S7pUNRcJb4ik9X?usp=sharing#scrollTo=nW4NEuImTzYu) Colab tutorial for compiling."
      ],
      "metadata": {
        "id": "j5MFRnmjOQ87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "nvcc -o fibonacci fibonacci.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDW2Y9vuxK-N",
        "outputId": "5848e8e7-ff67-4e6e-dff1-f669875ee7e4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Cell magic `%%cuda` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -I fibonacci -lcublas -lcusolver -Wno-deprecated-gpu-targets fibonacci.cu"
      ],
      "metadata": {
        "id": "7bjGlCKZNjay"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGxn-2r_OmV9",
        "outputId": "d7b36a57-6424-483b-ea3f-17c057f68420"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a.out  fibonacci.cu  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea1Gaa1HOpE8",
        "outputId": "2d3155ed-9ebe-41e6-9ea3-2874961ff69b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 100th Fibonacci number is: 0\n",
            "Elapsed time: 0.0000 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, calculate using a Python (recoursive, thus time optimized) calculation code."
      ],
      "metadata": {
        "id": "oUqyMoBdMIO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fibonacci series nth element calculator (python code)\n",
        "import time\n",
        "\n",
        "def nthFibonacci( n : int) -> int:\n",
        "    #recoursive calculation\n",
        "    mod_base = 10**9 +7 #for easier printing at high numbers\n",
        "    if n < 3:\n",
        "        return 1\n",
        "    arr1, arr2 = 0, 1\n",
        "    for _ in range(2, n+1, 1):\n",
        "        new_item = (arr1 + arr2) % mod_base\n",
        "        arr1, arr2 = arr2, new_item\n",
        "        print('Serie element #' + str(_) + ' : ' + str(new_item))\n",
        "    return arr2\n",
        "\n",
        "start_time = time.time()\n",
        "n = 50\n",
        "print('\\nSerie element #' + str(n) + ' : ' + str(nthFibonacci(n)))\n",
        "end_time = time.time()\n",
        "print(\"Elapsed time: \", (end_time - start_time)*1000) # 0.056 ms / cycle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYogeQgIIMh4",
        "outputId": "05ca4ef1-579e-478e-bd0a-c6e088fa4693"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serie element #2 : 1\n",
            "Serie element #3 : 2\n",
            "Serie element #4 : 3\n",
            "Serie element #5 : 5\n",
            "Serie element #6 : 8\n",
            "Serie element #7 : 13\n",
            "Serie element #8 : 21\n",
            "Serie element #9 : 34\n",
            "Serie element #10 : 55\n",
            "Serie element #11 : 89\n",
            "Serie element #12 : 144\n",
            "Serie element #13 : 233\n",
            "Serie element #14 : 377\n",
            "Serie element #15 : 610\n",
            "Serie element #16 : 987\n",
            "Serie element #17 : 1597\n",
            "Serie element #18 : 2584\n",
            "Serie element #19 : 4181\n",
            "Serie element #20 : 6765\n",
            "Serie element #21 : 10946\n",
            "Serie element #22 : 17711\n",
            "Serie element #23 : 28657\n",
            "Serie element #24 : 46368\n",
            "Serie element #25 : 75025\n",
            "Serie element #26 : 121393\n",
            "Serie element #27 : 196418\n",
            "Serie element #28 : 317811\n",
            "Serie element #29 : 514229\n",
            "Serie element #30 : 832040\n",
            "Serie element #31 : 1346269\n",
            "Serie element #32 : 2178309\n",
            "Serie element #33 : 3524578\n",
            "Serie element #34 : 5702887\n",
            "Serie element #35 : 9227465\n",
            "Serie element #36 : 14930352\n",
            "Serie element #37 : 24157817\n",
            "Serie element #38 : 39088169\n",
            "Serie element #39 : 63245986\n",
            "Serie element #40 : 102334155\n",
            "Serie element #41 : 165580141\n",
            "Serie element #42 : 267914296\n",
            "Serie element #43 : 433494437\n",
            "Serie element #44 : 701408733\n",
            "Serie element #45 : 134903163\n",
            "Serie element #46 : 836311896\n",
            "Serie element #47 : 971215059\n",
            "Serie element #48 : 807526948\n",
            "Serie element #49 : 778742000\n",
            "Serie element #50 : 586268941\n",
            "\n",
            "Serie element #50 : 586268941\n",
            "Elapsed time:  7.2345733642578125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Elapsed time: \", str(round(0.0156*n*1000,4)), \" ms\") # 0.056 ms / cycle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "178xqlCGLGVb",
        "outputId": "7b84372c-f1c5-48ca-d24a-3b85ef938291"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time:  780.0  ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without printing each step's result it may be faster, but not close to the CUDA run time."
      ],
      "metadata": {
        "id": "fE2EWhiLL8pK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exampe 5: Fractals with visualization\n",
        "https://colab.research.google.com/github/noahgift/cloud-data-analysis-at-scale/blob/master/GPU_Programming.ipynb"
      ],
      "metadata": {
        "id": "_K8oTV6XUHau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning\n",
        "If installed and not needed remove the git clone:"
      ],
      "metadata": {
        "id": "SalWkvNds5UI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmdir /s /q \"nvcc4jupyter\""
      ],
      "metadata": {
        "id": "LA4dQ8vFtAQ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}